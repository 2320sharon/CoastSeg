{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging Sessions Script\n",
    "\n",
    "This script is used to merge two or more sessions, provided they do not contain overlapping regions of interest (ROIs).\n",
    "\n",
    "### Prerequisites:\n",
    "- Paths to two session directories with extracted shorelines.\n",
    "- The desired name for the merged session directory that will be saved in the `sessions` directory.\n",
    "\n",
    "### Optional:\n",
    "- A `config.json` file with transect settings for calculating shoreline-transect intersections.\n",
    "\n",
    "### Instructions:\n",
    "1. Enter the paths to the session directories below:\n",
    "    ``` python\n",
    "   session_locations=[\n",
    "       '<path_to_first_session_directory>',\n",
    "       '<path_to_second_session_directory>'\n",
    "      ]\n",
    "    ```\n",
    "   Example:\n",
    "   - Notice that because these are Windows locations we put `r` at the beginning of each location\n",
    "    ``` python\n",
    "   session_locations=[\n",
    "      r'C:\\development\\doodleverse\\coastseg\\CoastSeg\\sessions\\es1\\ID_13_datetime06-05-23__04_16_45',\n",
    "      r'C:\\development\\doodleverse\\coastseg\\CoastSeg\\sessions\\es1\\ID_12_datetime06-05-23__04_16_45'\n",
    "      ]\n",
    "    ```\n",
    "2. Specify the name for the merged session directory:\n",
    "   - `merged_session_directory`: `\"<name_of_merged_session_directory>\"`\n",
    "\n",
    "3. (Optional) If you want to use your own advanced settings in a `config.json` file, include its path:\n",
    "   - `config_file`: `\"<path_to_config_json>\"`\n",
    "\n",
    "With the above information, the script can be executed to merge the specified sessions into a single session directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace these with the ROI directories from your own extract shorelines sessions\n",
    "\n",
    "session_locations=[r'C:\\development\\doodleverse\\coastseg\\CoastSeg\\sessions\\ID_rrw15_datetime11-21-23__11_32_09\\ID_rrw15_datetime11-21-23__11_32_09',\n",
    "                   r'C:\\development\\doodleverse\\coastseg\\CoastSeg\\sessions\\ID_rrw15_datetime11-21-23__11_35_25_es3\\ID_rrw15_datetime11-21-23__11_35_25']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_session_directory='merged_session_name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the merged session diretory under sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# enter the location of your sessions directory if this is not correct\n",
    "sessions_directory = os.path.join(os.getcwd(), 'sessions')\n",
    "print(sessions_directory)\n",
    "merged_session_location = os.path.join(sessions_directory, merged_session_directory)\n",
    "os.makedirs(merged_session_location, exist_ok=True)\n",
    "\n",
    "print(f\"Merged session will be saved to {merged_session_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shoreline-Transect Intersection Analysis Settings\n",
    "\n",
    "The default settings listed below should suffice for most use cases to find where extracted shorelines intersect transects. However, if you modified the advanced settings then you will need to adjust the settings.\n",
    "\n",
    "\n",
    "Most users will want to just use the default settings listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings_transects ={\n",
    "            \"along_dist\": 25,  # along-shore distance to use for computing the intersection\n",
    "            \"min_points\": 3,  # minimum number of shoreline points to calculate an intersection\n",
    "            \"max_std\": 15,  # max std for points around transect\n",
    "            \"max_range\": 30,  # max range for points around transect\n",
    "            \"min_chainage\": -100,  # largest negative value along transect (landwards of transect origin)\n",
    "            \"multiple_inter\": \"auto\",  # mode for removing outliers ('auto', 'nan', 'max')\n",
    "            \"prc_multiple\": 0.1,  # percentage of the time that multiple intersects are present to use the max\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from collections import defaultdict\n",
    "import os\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "# Related third party imports\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from shapely.geometry import LineString, MultiLineString, MultiPoint, Point\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "# Local application/library specific imports\n",
    "from coastseg import geodata_processing\n",
    "\n",
    "\n",
    "def convert_multipoints_to_linestrings(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Convert MultiPoint geometries in a GeoDataFrame to LineString geometries.\n",
    "\n",
    "    Args:\n",
    "    - gdf (gpd.GeoDataFrame): The input GeoDataFrame.\n",
    "\n",
    "    Returns:\n",
    "    - gpd.GeoDataFrame: A new GeoDataFrame with LineString geometries. If the input GeoDataFrame\n",
    "                        already contains LineStrings, the original GeoDataFrame is returned.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the GeoDataFrame\n",
    "    gdf_copy = gdf.copy()\n",
    "\n",
    "    # Check if all geometries in the gdf are LineStrings\n",
    "    if all(gdf_copy.geometry.type == \"LineString\"):\n",
    "        return gdf_copy\n",
    "\n",
    "    def multipoint_to_linestring(multipoint):\n",
    "        if isinstance(multipoint, MultiPoint):\n",
    "            return LineString(multipoint.geoms)\n",
    "        return multipoint\n",
    "\n",
    "    # Convert each MultiPoint to a LineString\n",
    "    gdf_copy[\"geometry\"] = gdf_copy[\"geometry\"].apply(multipoint_to_linestring)\n",
    "\n",
    "    return gdf_copy\n",
    "\n",
    "\n",
    "def dataframe_to_dict(df: pd.DataFrame, key_map: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Converts a DataFrame to a dictionary, with specific mapping between dictionary keys and DataFrame columns.\n",
    "\n",
    "    Parameters:\n",
    "    df : DataFrame\n",
    "        The DataFrame to convert.\n",
    "    key_map : dict\n",
    "        A dictionary where keys are the desired dictionary keys and values are the corresponding DataFrame column names.\n",
    "\n",
    "    Returns:\n",
    "    dict\n",
    "        The resulting dictionary.\n",
    "    \"\"\"\n",
    "    result_dict = defaultdict(list)\n",
    "\n",
    "    for dict_key, df_key in key_map.items():\n",
    "        if df_key in df.columns:\n",
    "            if df_key == \"date\":\n",
    "                # Assumes the column to be converted to date is the one specified in the mapping with key 'date'\n",
    "                result_dict[dict_key] = list(\n",
    "                    df[df_key].apply(\n",
    "                        lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        if pd.notnull(x)\n",
    "                        else None\n",
    "                    )\n",
    "                )\n",
    "            elif df_key == \"geometry\":\n",
    "                # Assumes the column to be converted to geometry is the one specified in the mapping with key 'geometry'\n",
    "                result_dict[dict_key] = list(\n",
    "                    df[df_key].apply(\n",
    "                        lambda x: np.array([list(point.coords[0]) for point in x.geoms])\n",
    "                        if pd.notnull(x)\n",
    "                        else None\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                result_dict[dict_key] = list(df[df_key])\n",
    "\n",
    "    return dict(result_dict)\n",
    "\n",
    "\n",
    "def convert_lines_to_multipoints(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Convert LineString or MultiLineString geometries in a GeoDataFrame to MultiPoint geometries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf : GeoDataFrame\n",
    "        The input GeoDataFrame containing LineString or MultiLineString geometries.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    GeoDataFrame\n",
    "        A new GeoDataFrame with MultiPoint geometries.\n",
    "\n",
    "    \"\"\"\n",
    "    # Create a copy of the input GeoDataFrame to avoid modifying it in place\n",
    "    gdf = gdf.copy()\n",
    "\n",
    "    # Define a function to convert LineString or MultiLineString to MultiPoint\n",
    "    def line_to_multipoint(geometry):\n",
    "        if isinstance(geometry, LineString):\n",
    "            return MultiPoint(geometry.coords)\n",
    "        elif isinstance(geometry, MultiLineString):\n",
    "            points = [MultiPoint(line.coords) for line in geometry.geoms]\n",
    "            return MultiPoint([point for multi in points for point in multi.geoms])\n",
    "        elif isinstance(geometry, MultiPoint):\n",
    "            return geometry\n",
    "        elif isinstance(geometry, Point):\n",
    "            return MultiPoint([geometry.coords])\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported geometry type: {type(geometry)}\")\n",
    "\n",
    "    # Apply the conversion function to each row in the GeoDataFrame\n",
    "    gdf[\"geometry\"] = gdf[\"geometry\"].apply(line_to_multipoint)\n",
    "\n",
    "    return gdf\n",
    "\n",
    "\n",
    "def read_first_geojson_file(\n",
    "    directory: str,\n",
    "    filenames=[\"extracted_shorelines_lines.geojson\", \"extracted_shorelines.geojson\"],\n",
    "):\n",
    "    # Loop over the filenames\n",
    "    for filename in filenames:\n",
    "        filepath = os.path.join(directory, filename)\n",
    "\n",
    "        # If the file exists, read it and return the GeoDataFrame\n",
    "        if os.path.exists(filepath):\n",
    "            return geodata_processing.read_gpd_file(filepath)\n",
    "\n",
    "    # If none of the files exist, raise an exception\n",
    "    raise FileNotFoundError(\n",
    "        f\"None of the files {filenames} exist in the directory {directory}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def clip_gdfs(gdfs, overlap_gdf):\n",
    "    \"\"\"\n",
    "    Clips GeoDataFrames to an overlapping region.\n",
    "\n",
    "    Parameters:\n",
    "    gdfs : list of GeoDataFrames\n",
    "        The GeoDataFrames to be clipped.\n",
    "    overlap_gdf : GeoDataFrame\n",
    "        The overlapping region to which the GeoDataFrames will be clipped.\n",
    "\n",
    "    Returns:\n",
    "    list of GeoDataFrames\n",
    "        The clipped GeoDataFrames.\n",
    "    \"\"\"\n",
    "    clipped_gdfs = []\n",
    "    for gdf in gdfs:\n",
    "        clipped_gdf = gpd.clip(gdf, overlap_gdf)\n",
    "        if not clipped_gdf.empty:\n",
    "            clipped_gdfs.append(clipped_gdf)\n",
    "            clipped_gdf.plot()\n",
    "    return clipped_gdfs\n",
    "\n",
    "\n",
    "def calculate_overlap(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the intersection of all pairs of polygons in a GeoDataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    gdf : GeoDataFrame\n",
    "        A GeoDataFrame containing polygons.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    overlap_gdf : GeoDataFrame\n",
    "        A GeoDataFrame containing the intersection of all pairs of polygons in gdf.\n",
    "    \"\"\"\n",
    "    # Check if the input GeoDataFrame is empty\n",
    "    if not hasattr(gdf, \"empty\"):\n",
    "        return gpd.GeoDataFrame()\n",
    "    if gdf.empty:\n",
    "        # Return an empty GeoDataFrame with the same CRS if it exists\n",
    "        return gpd.GeoDataFrame(\n",
    "            geometry=[], crs=gdf.crs if hasattr(gdf, \"crs\") else None\n",
    "        )\n",
    "\n",
    "    # Initialize a list to store the intersections\n",
    "    intersections = []\n",
    "\n",
    "    # Loop over each pair of rows in gdf\n",
    "    for i in range(len(gdf) - 1):\n",
    "        for j in range(i + 1, len(gdf)):\n",
    "            # Check for intersection\n",
    "            if gdf.iloc[i].geometry.intersects(gdf.iloc[j].geometry):\n",
    "                # Calculate the intersection\n",
    "                intersection = gdf.iloc[i].geometry.intersection(gdf.iloc[j].geometry)\n",
    "                # Append the intersection to the intersections list\n",
    "                intersections.append(intersection)\n",
    "\n",
    "    # Create a GeoSeries from the intersections\n",
    "    intersection_series = gpd.GeoSeries(intersections, crs=gdf.crs)\n",
    "\n",
    "    # Create a GeoDataFrame from the GeoSeries\n",
    "    overlap_gdf = gpd.GeoDataFrame(geometry=intersection_series)\n",
    "    return overlap_gdf\n",
    "\n",
    "\n",
    "def average_multipoints(multipoints) -> MultiPoint:\n",
    "    \"\"\"\n",
    "    Calculate the average MultiPoint geometry from a list of MultiPoint geometries.\n",
    "\n",
    "    This function takes a list of shapely MultiPoint geometries, ensures they all have the same number of points\n",
    "    by padding shorter MultiPoints with their last point, and then calculates the average coordinates\n",
    "    for each point position across all the input MultiPoint geometries.\n",
    "\n",
    "    The result is a new MultiPoint geometry that represents the average shape of the input MultiPoints.\n",
    "\n",
    "    Parameters:\n",
    "    multipoints (list of shapely.geometry.MultiPoint): A list of shapely MultiPoint geometries to be averaged.\n",
    "\n",
    "    Returns:\n",
    "    shapely.geometry.MultiPoint: A MultiPoint geometry representing the average shape of the input MultiPoints.\n",
    "\n",
    "    Raises:\n",
    "    ValueError: If the input list of MultiPoint geometries is empty.\n",
    "\n",
    "    Example:\n",
    "    >>> from shapely.geometry import MultiPoint\n",
    "    >>> multipoint1 = MultiPoint([(0, 0), (1, 1), (2, 2)])\n",
    "    >>> multipoint2 = MultiPoint([(1, 1), (2, 2)])\n",
    "    >>> multipoint3 = MultiPoint([(0, 0), (1, 1), (2, 2), (3, 3)])\n",
    "    >>> average_mp = average_multipoints([multipoint1, multipoint2, multipoint3])\n",
    "    >>> print(average_mp)\n",
    "    MULTIPOINT (0.3333333333333333 0.3333333333333333, 1.3333333333333333 1.3333333333333333, 2 2, 3 3)\n",
    "    \"\"\"\n",
    "    if not multipoints:\n",
    "        raise ValueError(\"The list of MultiPoint geometries is empty\")\n",
    "\n",
    "    # Find the maximum number of points in any MultiPoint\n",
    "    max_len = max(len(mp.geoms) for mp in multipoints)\n",
    "\n",
    "    # Pad shorter MultiPoints with their last point\n",
    "    padded_multipoints = []\n",
    "    for mp in multipoints:\n",
    "        if len(mp.geoms) < max_len:\n",
    "            padded_multipoints.append(\n",
    "                MultiPoint(list(mp.geoms) + [mp.geoms[-1]] * (max_len - len(mp.geoms)))\n",
    "            )\n",
    "        else:\n",
    "            padded_multipoints.append(mp)\n",
    "\n",
    "    # Calculate the average coordinates for each point\n",
    "    num_multipoints = len(padded_multipoints)\n",
    "    average_coords = []\n",
    "    for i in range(max_len):\n",
    "        avg_left = sum(mp.geoms[i].x for mp in padded_multipoints) / num_multipoints\n",
    "        avg_right = sum(mp.geoms[i].y for mp in padded_multipoints) / num_multipoints\n",
    "        average_coords.append((avg_left, avg_right))\n",
    "\n",
    "    return MultiPoint(average_coords)\n",
    "\n",
    "\n",
    "def merge_geometries(merged_gdf, columns=None, operation=unary_union):\n",
    "    \"\"\"\n",
    "    Performs a specified operation for the geometries with the same date and satname.\n",
    "\n",
    "    Parameters:\n",
    "    merged_gdf : GeoDataFrame\n",
    "        The GeoDataFrame to perform the operation on.\n",
    "    columns : list of str, optional\n",
    "        The columns to perform the operation on. If None, all columns with 'geometry' in the name are used.\n",
    "    operation : function, optional\n",
    "        The operation to perform. If None, unary_union is used.\n",
    "\n",
    "    Returns:\n",
    "    GeoDataFrame\n",
    "        The GeoDataFrame with the operation performed.\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        columns = [col for col in merged_gdf.columns if \"geometry\" in col]\n",
    "    else:\n",
    "        columns = [col for col in columns if col in merged_gdf.columns]\n",
    "\n",
    "    merged_gdf[\"geometry\"] = merged_gdf[columns].apply(\n",
    "        lambda row: operation(row.tolist()), axis=1\n",
    "    )\n",
    "    for col in columns:\n",
    "        if col in merged_gdf.columns and col != \"geometry\":\n",
    "            merged_gdf = merged_gdf.drop(columns=col)\n",
    "    return merged_gdf\n",
    "\n",
    "\n",
    "def read_geojson_files(filepaths):\n",
    "    \"\"\"Read GeoJSON files into GeoDataFrames and return a list.\"\"\"\n",
    "    return [gpd.read_file(path) for path in filepaths]\n",
    "\n",
    "\n",
    "def concatenate_gdfs(gdfs):\n",
    "    \"\"\"Concatenate a list of GeoDataFrames into a single GeoDataFrame.\"\"\"\n",
    "    return pd.concat(gdfs, ignore_index=True)\n",
    "\n",
    "\n",
    "def filter_and_join_gdfs(gdf, feature_type, predicate=\"intersects\"):\n",
    "    \"\"\"Filter GeoDataFrame by feature type, ensure spatial index, and perform a spatial join.\"\"\"\n",
    "    if \"type\" not in gdf.columns:\n",
    "        raise ValueError(\"The GeoDataFrame must contain a column named 'type'\")\n",
    "    filtered_gdf = gdf[gdf[\"type\"] == feature_type].copy()[[\"geometry\"]]\n",
    "    filtered_gdf[\"geometry\"] = filtered_gdf[\"geometry\"].simplify(\n",
    "        tolerance=0.001\n",
    "    )  # Simplify geometry if possible to improve performance\n",
    "    filtered_gdf.sindex  # Ensure spatial index\n",
    "    return gpd.sjoin(gdf, filtered_gdf[[\"geometry\"]], how=\"inner\", predicate=predicate)\n",
    "\n",
    "\n",
    "def aggregate_gdf(gdf: gpd.GeoDataFrame, group_fields: list) -> gpd.GeoDataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate a GeoDataFrame by specified fields using a custom combination function.\n",
    "\n",
    "    Parameters:\n",
    "        gdf (GeoDataFrame): The input GeoDataFrame to be aggregated.\n",
    "        group_fields (list): The fields to group the GeoDataFrame by.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: The aggregated GeoDataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def combine_non_nulls(series):\n",
    "        unique_values = series.dropna().unique()\n",
    "        return (\n",
    "            unique_values[0]\n",
    "            if len(unique_values) == 1\n",
    "            else \", \".join(map(str, unique_values))\n",
    "        )\n",
    "\n",
    "    if \"index_right\" in gdf.columns:\n",
    "        gdf = gdf.drop(columns=[\"index_right\"])\n",
    "\n",
    "    return (\n",
    "        gdf.drop_duplicates()\n",
    "        .groupby(group_fields, as_index=False)\n",
    "        .agg(combine_non_nulls)\n",
    "    )\n",
    "\n",
    "\n",
    "def merge_geojson_files(session_locations, merged_session_location):\n",
    "    \"\"\"Main function to merge GeoJSON files from different session locations.\"\"\"\n",
    "    filepaths = [\n",
    "        os.path.join(location, \"config_gdf.geojson\") for location in session_locations\n",
    "    ]\n",
    "    gdfs = read_geojson_files(filepaths)\n",
    "    merged_gdf = gpd.GeoDataFrame(concatenate_gdfs(gdfs), geometry=\"geometry\")\n",
    "\n",
    "    # Filter the geodataframe to only elements that intersect with the rois (dramatically drops the size of the geodataframe)\n",
    "    merged_config = filter_and_join_gdfs(merged_gdf, \"roi\", predicate=\"intersects\")\n",
    "    # apply a group by operation to combine the rows with the same type and geometry into a single row\n",
    "    merged_config = aggregate_gdf(merged_config, [\"type\", \"geometry\"])\n",
    "    # applying the group by function in aggregate_gdf() turns the geodataframe into a dataframe\n",
    "    merged_config = gpd.GeoDataFrame(merged_config, geometry=\"geometry\")\n",
    "\n",
    "    output_path = os.path.join(merged_session_location, \"merged_config.geojson\")\n",
    "    merged_config.to_file(output_path, driver=\"GeoJSON\")\n",
    "\n",
    "    return merged_config\n",
    "\n",
    "\n",
    "def create_csv_per_transect(\n",
    "    save_path: str,\n",
    "    cross_distance_transects: dict,\n",
    "    extracted_shorelines_dict: dict,\n",
    "    roi_id: str = None,  # ROI ID is now optional and defaults to None\n",
    "    filename_suffix: str = \"_timeseries_raw.csv\",\n",
    "):\n",
    "    for key, distances in cross_distance_transects.items():\n",
    "        # Initialize the dictionary for DataFrame with mandatory keys\n",
    "        data_dict = {\n",
    "            \"dates\": extracted_shorelines_dict[\"dates\"],\n",
    "            \"satname\": extracted_shorelines_dict[\"satname\"],\n",
    "            key: distances,\n",
    "        }\n",
    "\n",
    "        # Add roi_id to the dictionary if provided\n",
    "        if roi_id is not None:\n",
    "            data_dict[\"roi_id\"] = [roi_id] * len(extracted_shorelines_dict[\"dates\"])\n",
    "\n",
    "        # Create a DataFrame directly with the data dictionary\n",
    "        df = pd.DataFrame(data_dict).set_index(\"dates\")\n",
    "\n",
    "        # Construct the full file path\n",
    "        csv_filename = f\"{key}{filename_suffix}\"\n",
    "        fn = os.path.join(save_path, csv_filename)\n",
    "\n",
    "        # Save to CSV file, 'mode' set to 'w' for overwriting\n",
    "        try:\n",
    "            df.to_csv(fn, sep=\",\", mode=\"w\")\n",
    "            print(f\"Time-series for transect {key} saved to {fn}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save time-series for transect {key}: {e}\")\n",
    "\n",
    "\n",
    "def merge_and_average(df1: gpd.GeoDataFrame, df2: gpd.GeoDataFrame) -> gpd.GeoDataFrame:\n",
    "    # Perform a full outer join\n",
    "    merged = pd.merge(\n",
    "        df1, df2, on=[\"satname\", \"date\"], how=\"outer\", suffixes=(\"_df1\", \"_df2\")\n",
    "    )\n",
    "\n",
    "    # Identify numeric columns from both dataframes\n",
    "    numeric_columns_df1 = df1.select_dtypes(include=\"number\").columns\n",
    "    numeric_columns_df2 = df2.select_dtypes(include=\"number\").columns\n",
    "    common_numeric_columns = set(numeric_columns_df1).intersection(numeric_columns_df2)\n",
    "\n",
    "    # Average the numeric columns\n",
    "    for column in common_numeric_columns:\n",
    "        merged[column] = merged[[f\"{column}_df1\", f\"{column}_df2\"]].mean(axis=1)\n",
    "\n",
    "    # Drop the original numeric columns\n",
    "    merged.drop(\n",
    "        columns=[f\"{column}_df1\" for column in common_numeric_columns]\n",
    "        + [f\"{column}_df2\" for column in common_numeric_columns],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    # Merge geometries\n",
    "    geometry_columns = [col for col in merged.columns if \"geometry\" in col]\n",
    "    merged = merge_geometries(merged, columns=geometry_columns)\n",
    "\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all the config_gdf.geojson files together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the shorelines or transects are at the exact same location, they will be merged into one\n",
    "# if transects have different ids for the same location, they will be merged into one and both ids will be saved\n",
    "\n",
    "merged_config  = merge_geojson_files(session_locations, merged_session_location)\n",
    "merged_config "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ROI Listed Below Will be Merged Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_rows = merged_config[merged_config['type'] == 'roi']\n",
    "roi_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge the Extracted Shorelines Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coastseg.merge_utils import calculate_overlap, clip_gdfs,  read_first_geojson_file, convert_lines_to_multipoints,merge_and_average\n",
    "from functools import reduce\n",
    "\n",
    "combined_gdf = gpd.GeoDataFrame( geometry=[], crs='epsg:4326')\n",
    "# calculate the overlapping regions between the ROIs\n",
    "overlap_gdf=calculate_overlap(roi_rows)\n",
    "\n",
    "# read all the extracted shorelines from the session locations\n",
    "gdfs = []\n",
    "for session_dir in session_locations:\n",
    "    # attempt to read the extracted shoreline files\n",
    "    es_gdf = read_first_geojson_file(session_dir,['extracted_shorelines_points.geojson', 'extracted_shorelines.geojson'])\n",
    "    es_gdf = convert_lines_to_multipoints(es_gdf)\n",
    "    es_gdf = es_gdf.to_crs('epsg:4326')\n",
    "    gdfs.append(es_gdf)\n",
    "print(f\"Read {len(gdfs)} extracted shorelines GeoDataFrames\")\n",
    "\n",
    "# clip the extracted shorelines to the overlapping regions\n",
    "clipped_shorelines_gdfs=clip_gdfs(gdfs, overlap_gdf)\n",
    "\n",
    "# sometimes there are not shorelines in the overlapping regions\n",
    "if overlap_gdf.empty or len(clipped_shorelines_gdfs) == 0:\n",
    "    print(\"No overlapping ROIs found. Sessions can be merged.\")\n",
    "    # merge the geodataframes on date and satname and average the cloud_cover and geoaccuracy for the merged rows\n",
    "\n",
    "    for gdf in gdfs:\n",
    "        if not gdf.crs:\n",
    "            gdf.set_crs(\"EPSG:4326\", inplace=True)\n",
    "        \n",
    "    # Perform a full outer join and average the numeric columns across all GeoDataFrames\n",
    "    result = reduce(merge_and_average, gdfs)\n",
    "\n",
    "    result.sort_values(by='date', inplace=True)\n",
    "    result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(f\"Combined {len(result)} rows from {len(gdfs)} GeoDataFrames\")\n",
    "print(f\"The following dataframe contains the combined extracted shorelines from all sessions.\\n Shorelines that were extracted on the same dates have been combined.\")\n",
    "\n",
    "\n",
    "combined_gdf = result\n",
    "combined_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Merged Extracted Shorelines to a JSON file\n",
    "- This will contains all the metadata for each extracted shoreline such as \n",
    "\n",
    "\n",
    "      1. cloud cover\n",
    "      2. date\n",
    "      3. satellite it was derived from \n",
    "      4. geoaccuracy\n",
    "- Filename: `extracted_shorelines_dict.json`\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coastseg import file_utilities\n",
    "\n",
    "# mapping of dictionary keys to dataframe columns\n",
    "keymap ={'shorelines':'geometry',\n",
    "         'dates':'date',\n",
    "         'satname':'satname',\n",
    "         'cloud_cover':'cloud_cover',\n",
    "         'geoaccuracy':'geoaccuracy'}\n",
    "# shoreline dict should have keys: dates, satname, cloud_cover, geoaccuracy, shorelines\n",
    "shoreline_dict = dataframe_to_dict(combined_gdf,keymap)\n",
    "# save the extracted shoreline dictionary to json file\n",
    "file_utilities.to_file(shoreline_dict, os.path.join(merged_session_location, \"extracted_shorelines_dict.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Number of Extracted Shorelines Across All ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shoreline_dict['shorelines'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Merged Extracted Shorelines to GeoJSON Files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coastseg.common import convert_linestrings_to_multipoints, stringify_datetime_columns\n",
    "import os\n",
    "# Save extracted shorelines as a GeoJSON file\n",
    "es_line_path = os.path.join(merged_session_location, \"extracted_shorelines_lines.geojson\")\n",
    "es_pts_path = os.path.join(merged_session_location, \"extracted_shorelines_points.geojson\")\n",
    "\n",
    "es_lines_gdf = convert_multipoints_to_linestrings(combined_gdf)\n",
    "# save extracted shorelines as interpolated linestrings\n",
    "es_lines_gdf.to_file(es_line_path, driver='GeoJSON')\n",
    "\n",
    "\n",
    "points_gdf = convert_linestrings_to_multipoints(combined_gdf)\n",
    "points_gdf = stringify_datetime_columns(points_gdf)\n",
    "# Save extracted shorelines as mulitpoints GeoJSON file\n",
    "points_gdf.to_file(es_pts_path, driver='GeoJSON')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Find when the Transects and Shorelines intersect\n",
    "1. Loads the Transects for all the ROIs \n",
    "2. Get the shoreline dictionary we created earlier and read the shorelines from it\n",
    "3. Find where the shorelines and transects intersect\n",
    "4. Save the shoreline and transect intersections as a timeseries to a csv file\n",
    "5. Save the timeseries of intersections between the shoreline and a single tranesct to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coastsat import SDS_transects\n",
    "# 1. load transects for all ROIs\n",
    "transect_rows = merged_config[merged_config['type'] == 'transect']\n",
    "transects_dict = {row['id']: np.array(row[\"geometry\"].coords) for i, row in transect_rows.iterrows()}\n",
    "# 2. compute the intersection between the transects and the extracted shorelines\n",
    "cross_distance = SDS_transects.compute_intersection_QC(shoreline_dict, transects_dict, settings_transects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coastseg.common import get_cross_distance_df\n",
    "# use coastseg.common to get the cross_distance_df\n",
    "transects_df = get_cross_distance_df(shoreline_dict,cross_distance)\n",
    "# save the transect shoreline intersections to csv timeseries file\n",
    "filepath = os.path.join(merged_session_location, \"transect_time_series.csv\")\n",
    "transects_df.to_csv(filepath, sep=\",\")\n",
    "transects_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save a CSV for Each Transect \n",
    "- WARNING some of these transects will contain a lot of null values because they don't intersect with other ROI's extracted shorelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the timeseries of intersections between the shoreline and a single tranesct to csv file\n",
    "create_csv_per_transect(merged_session_location,cross_distance,shoreline_dict,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
